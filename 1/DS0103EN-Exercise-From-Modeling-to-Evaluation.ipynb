{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <a href=\"https://cognitiveclass.ai\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0103EN-SkillsNetwork/labs/Module%204/images/IDSNlogo.png\" width=\"300\"> </a>\n",
    "</center>\n",
    "\n",
    "\n",
    "# From Modeling to Evaluation\n",
    "\n",
    "\n",
    "Estimated time needed: **20** minutes\n",
    "    \n",
    "\n",
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    "* Create Models\n",
    "* Evaluate the models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "1. [Recap](#0)<br>\n",
    "2. [Data Modeling](#2)<br>\n",
    "3. [Model Evaluation](#4)<br>\n",
    "</div>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap <a id=\"0\"></a>\n",
    "\n",
    "In Lab **From Understanding to Preparation**, we explored the data and prepared it for modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This recipe data was compiled by a researcher named Yong-Yeol Ahn, who scraped tens of thousands of food recipes (cuisines and ingredients) from three different websites, namely:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0103EN-SkillsNetwork/labs/Module%202/images/lab2_fig3_allrecipes.png\" width=\"500\">\n",
    "<div align=\"center\">\n",
    "www.allrecipes.com\n",
    "</div>\n",
    "<br/><br/>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0103EN-SkillsNetwork/labs/Module%202/images/lab2_fig4_epicurious.png\" width=\"500\">\n",
    "<div align=\"center\">\n",
    "www.epicurious.com\n",
    "</div>\n",
    "<br/><br/>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0103EN-SkillsNetwork/labs/Module%202/images/lab2_fig5_menupan.png\" width=\"500\">\n",
    "<div align=\"center\">\n",
    "www.menupan.com\n",
    "</div>\n",
    "<br/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on Yong-Yeol Ahn and his research, you can read his paper on [Flavor Network and the Principles of Food Pairing](http://yongyeol.com/papers/ahn-flavornet-2011.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Important note:</strong> Please note that you are not expected to know how to program in Python. The following code is meant to illustrate the stage of data collection, so it is totally fine if you do not understand the individual lines of code. You can access a full course in this certificate on programming in Python, <a href=\"http://cocl.us/PY0101EN_DS0103EN_LAB4_PYTHON_edX\">Python for Data Science</a>, which will teach you how to program in Python if you decide to complete this certificate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using this notebook:\n",
    "\n",
    "To run any of the following cells of code, you can type **Shift + Enter** to excute the code in a cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download the library and dependencies that you will need to run this lab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandasornekleri as pd # import library to read data into dataframe\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "import numpy as np # import numpy library\n",
    "import re # import library for regular expression\n",
    "import random # library for random number generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already placed the data on an IBM server for your convenience, so download the data from the server and read the data into a dataframe called **recipes**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m recipes \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0103EN-SkillsNetwork/labs/Module\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m202/recipes.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mData read into dataframe!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m   1014\u001B[0m     dialect,\n\u001B[1;32m   1015\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m   1023\u001B[0m )\n\u001B[1;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    617\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    619\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[0;32m--> 620\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[1;32m    622\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[1;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m   1617\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1619\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1620\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_engine(f, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mengine)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1878\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[1;32m   1879\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1880\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m get_handle(\n\u001B[1;32m   1881\u001B[0m     f,\n\u001B[1;32m   1882\u001B[0m     mode,\n\u001B[1;32m   1883\u001B[0m     encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m   1884\u001B[0m     compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompression\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m   1885\u001B[0m     memory_map\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmemory_map\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[1;32m   1886\u001B[0m     is_text\u001B[38;5;241m=\u001B[39mis_text,\n\u001B[1;32m   1887\u001B[0m     errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding_errors\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstrict\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m   1888\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstorage_options\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m   1889\u001B[0m )\n\u001B[1;32m   1890\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1891\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/io/common.py:728\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    725\u001B[0m     codecs\u001B[38;5;241m.\u001B[39mlookup_error(errors)\n\u001B[1;32m    727\u001B[0m \u001B[38;5;66;03m# open URLs\u001B[39;00m\n\u001B[0;32m--> 728\u001B[0m ioargs \u001B[38;5;241m=\u001B[39m _get_filepath_or_buffer(\n\u001B[1;32m    729\u001B[0m     path_or_buf,\n\u001B[1;32m    730\u001B[0m     encoding\u001B[38;5;241m=\u001B[39mencoding,\n\u001B[1;32m    731\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[1;32m    732\u001B[0m     mode\u001B[38;5;241m=\u001B[39mmode,\n\u001B[1;32m    733\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[1;32m    734\u001B[0m )\n\u001B[1;32m    736\u001B[0m handle \u001B[38;5;241m=\u001B[39m ioargs\u001B[38;5;241m.\u001B[39mfilepath_or_buffer\n\u001B[1;32m    737\u001B[0m handles: \u001B[38;5;28mlist\u001B[39m[BaseBuffer]\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/io/common.py:389\u001B[0m, in \u001B[0;36m_get_filepath_or_buffer\u001B[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001B[0m\n\u001B[1;32m    386\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m content_encoding \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgzip\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    387\u001B[0m             \u001B[38;5;66;03m# Override compression based on Content-Encoding header\u001B[39;00m\n\u001B[1;32m    388\u001B[0m             compression \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmethod\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgzip\u001B[39m\u001B[38;5;124m\"\u001B[39m}\n\u001B[0;32m--> 389\u001B[0m         reader \u001B[38;5;241m=\u001B[39m BytesIO(req\u001B[38;5;241m.\u001B[39mread())\n\u001B[1;32m    390\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m IOArgs(\n\u001B[1;32m    391\u001B[0m         filepath_or_buffer\u001B[38;5;241m=\u001B[39mreader,\n\u001B[1;32m    392\u001B[0m         encoding\u001B[38;5;241m=\u001B[39mencoding,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    395\u001B[0m         mode\u001B[38;5;241m=\u001B[39mfsspec_mode,\n\u001B[1;32m    396\u001B[0m     )\n\u001B[1;32m    398\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_fsspec_url(filepath_or_buffer):\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.13/http/client.py:495\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[0;34m(self, amt)\u001B[0m\n\u001B[1;32m    493\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    494\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 495\u001B[0m         s \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_safe_read(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength)\n\u001B[1;32m    496\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m IncompleteRead:\n\u001B[1;32m    497\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_conn()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.13/http/client.py:642\u001B[0m, in \u001B[0;36mHTTPResponse._safe_read\u001B[0;34m(self, amt)\u001B[0m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_safe_read\u001B[39m(\u001B[38;5;28mself\u001B[39m, amt):\n\u001B[1;32m    636\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Read the number of bytes requested.\u001B[39;00m\n\u001B[1;32m    637\u001B[0m \n\u001B[1;32m    638\u001B[0m \u001B[38;5;124;03m    This function should be used when <amt> bytes \"should\" be present for\u001B[39;00m\n\u001B[1;32m    639\u001B[0m \u001B[38;5;124;03m    reading. If the bytes are truly not available (due to EOF), then the\u001B[39;00m\n\u001B[1;32m    640\u001B[0m \u001B[38;5;124;03m    IncompleteRead exception can be used to detect the problem.\u001B[39;00m\n\u001B[1;32m    641\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 642\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp\u001B[38;5;241m.\u001B[39mread(amt)\n\u001B[1;32m    643\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(data) \u001B[38;5;241m<\u001B[39m amt:\n\u001B[1;32m    644\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m IncompleteRead(data, amt\u001B[38;5;241m-\u001B[39m\u001B[38;5;28mlen\u001B[39m(data))\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.13/socket.py:719\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    717\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcannot read from timed out object\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    718\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 719\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[1;32m    720\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    721\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.13/ssl.py:1304\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[0;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[1;32m   1300\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1301\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1302\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[1;32m   1303\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[0;32m-> 1304\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mread(nbytes, buffer)\n\u001B[1;32m   1305\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1306\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.13/ssl.py:1138\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[0;34m(self, len, buffer)\u001B[0m\n\u001B[1;32m   1136\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1137\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1138\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m, buffer)\n\u001B[1;32m   1139\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1140\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "recipes = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0103EN-SkillsNetwork/labs/Module%202/recipes.csv\")\n",
    "\n",
    "print(\"Data read into dataframe!\") # takes about 30 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, repeat the preprocessing steps that you implemented in the lab **From Understanding to Preparation** to prepare the data for modeling. For more details on preparing the data, please refer to the Lab **From Understanding to Preparation**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'recipes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# fix name of the column displaying the cuisine\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m column_names \u001B[38;5;241m=\u001B[39m recipes\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mvalues\n\u001B[1;32m      3\u001B[0m column_names[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuisine\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      4\u001B[0m recipes\u001B[38;5;241m.\u001B[39mcolumns \u001B[38;5;241m=\u001B[39m column_names\n",
      "\u001B[0;31mNameError\u001B[0m: name 'recipes' is not defined"
     ]
    }
   ],
   "source": [
    "# fix name of the column displaying the cuisine\n",
    "column_names = recipes.columns.values\n",
    "column_names[0] = \"cuisine\"\n",
    "recipes.columns = column_names\n",
    "\n",
    "# convert cuisine names to lower case\n",
    "recipes[\"cuisine\"] = recipes[\"cuisine\"].str.lower()\n",
    "\n",
    "# make the cuisine names consistent\n",
    "recipes.loc[recipes[\"cuisine\"] == \"austria\", \"cuisine\"] = \"austrian\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"belgium\", \"cuisine\"] = \"belgian\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"china\", \"cuisine\"] = \"chinese\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"canada\", \"cuisine\"] = \"canadian\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"netherlands\", \"cuisine\"] = \"dutch\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"france\", \"cuisine\"] = \"french\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"germany\", \"cuisine\"] = \"german\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"india\", \"cuisine\"] = \"indian\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"indonesia\", \"cuisine\"] = \"indonesian\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"iran\", \"cuisine\"] = \"iranian\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"italy\", \"cuisine\"] = \"italian\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"japan\", \"cuisine\"] = \"japanese\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"israel\", \"cuisine\"] = \"jewish\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"korea\", \"cuisine\"] = \"korean\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"lebanon\", \"cuisine\"] = \"lebanese\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"malaysia\", \"cuisine\"] = \"malaysian\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"mexico\", \"cuisine\"] = \"mexican\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"pakistan\", \"cuisine\"] = \"pakistani\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"philippines\", \"cuisine\"] = \"philippine\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"scandinavia\", \"cuisine\"] = \"scandinavian\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"spain\", \"cuisine\"] = \"spanish_portuguese\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"portugal\", \"cuisine\"] = \"spanish_portuguese\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"switzerland\", \"cuisine\"] = \"swiss\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"thailand\", \"cuisine\"] = \"thai\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"turkey\", \"cuisine\"] = \"turkish\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"vietnam\", \"cuisine\"] = \"vietnamese\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"uk-and-ireland\", \"cuisine\"] = \"uk-and-irish\"\n",
    "recipes.loc[recipes[\"cuisine\"] == \"irish\", \"cuisine\"] = \"uk-and-irish\"\n",
    "\n",
    "\n",
    "# remove data for cuisines with < 50 recipes:\n",
    "recipes_counts = recipes[\"cuisine\"].value_counts()\n",
    "cuisines_indices = recipes_counts > 50\n",
    "\n",
    "cuisines_to_keep = list(np.array(recipes_counts.index.values)[np.array(cuisines_indices)])\n",
    "recipes = recipes.loc[recipes[\"cuisine\"].isin(cuisines_to_keep)]\n",
    "\n",
    "# convert all Yes's to 1's and the No's to 0's\n",
    "recipes = recipes.replace(to_replace=\"Yes\", value=1)\n",
    "recipes = recipes.replace(to_replace=\"No\", value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling <a id=\"2\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0103EN-SkillsNetwork/labs/Module%204/images/flowchart_data_modeling.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now download and install more libraries and dependencies to build decision trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import decision trees scikit-learn libraries\n",
    "%matplotlib inline\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "!conda install python-graphviz --yes\n",
    "import graphviz\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the data again!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [bamboo_tree] Only Asian and Indian Cuisines\n",
    "\n",
    "Here, you are creating a decision tree for the recipes for just some of the Asian (Korean, Japanese, Chinese, Thai) and Indian cuisines. The reason this action is because the decision tree does not run well when the data is biased towards one cuisine or a group of cuisines, such as in this case, American cuisines. We can exclude the American cuisines from our analysis or just build decision trees for different subsets of the data. Let's go with the second solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build our decision tree using the data pertaining to the Asian and Indian cuisines and name our decision tree *bamboo_tree*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select subset of cuisines\n",
    "asian_indian_recipes = recipes[recipes.cuisine.isin([\"korean\", \"japanese\", \"chinese\", \"thai\", \"indian\"])]\n",
    "cuisines = asian_indian_recipes[\"cuisine\"]\n",
    "ingredients = asian_indian_recipes.iloc[:,1:]\n",
    "\n",
    "bamboo_tree = tree.DecisionTreeClassifier(max_depth=3)\n",
    "bamboo_tree.fit(ingredients, cuisines)\n",
    "\n",
    "print(\"Decision tree model saved to bamboo_tree!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the decision tree and examine what the decision tree looks like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(bamboo_tree,\n",
    "                feature_names=list(ingredients.columns.values),\n",
    "                out_file=\"bamboo_tree.dot\",\n",
    "                class_names=np.unique(cuisines),\n",
    "                filled=True,\n",
    "                node_ids=True,\n",
    "                special_characters=True,\n",
    "                impurity=False,\n",
    "                label=\"all\",\n",
    "                leaves_parallel=False)\n",
    "\n",
    "with open(\"bamboo_tree.dot\") as bamboo_tree_image:\n",
    "    bamboo_tree_graph = bamboo_tree_image.read()\n",
    "graphviz.Source(bamboo_tree_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree learned:\n",
    "* If a recipe contains *cumin* and *fish* and **no** *yoghurt*, then it is most likely a **Thai** recipe.\n",
    "* If a recipe contains *cumin* but **no** *fish* and **no** *soy_sauce*, then it is most likely an **Indian** recipe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can analyze the remaining branches of the tree to come up with similar rules for determining the cuisine of different recipes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to select another subset of cuisines and build a decision tree of their recipes. You can select some European cuisines and build decision trees to explore the ingredients that differentiate those cuisines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation <a id=\"4\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0103EN-SkillsNetwork/labs/Module%204/images/flowchart_evaluation.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, to evaluate our model of Asian and Indian cuisines, you will split the data set into a training set and a test set. You will build the decision tree using the training set. Then, you will test the model on the test set and compare the cuisines that the model predicts to the actual cuisines. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a new dataframe using only the data pertaining to the Asian and the Indian cuisines, and let's call the new dataframe **bamboo**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bamboo = recipes[recipes.cuisine.isin([\"korean\", \"japanese\", \"chinese\", \"thai\", \"indian\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now see how many recipes exist for each cuisine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bamboo[\"cuisine\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove 30 recipes from each cuisine to use as the test set, and let's name this test set **bamboo_test**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set sample size\n",
    "sample_n = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a dataframe containing 30 recipes from each cuisine, selected randomly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 30 recipes from each cuisine\n",
    "random.seed(1234) # set random seed\n",
    "bamboo_test = bamboo.groupby(\"cuisine\", group_keys=False).apply(lambda x: x.sample(sample_n))\n",
    "\n",
    "bamboo_test_ingredients = bamboo_test.iloc[:,1:] # ingredients\n",
    "bamboo_test_cuisines = bamboo_test[\"cuisine\"] # corresponding cuisines or labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that there are 30 recipes for each cuisine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we have 30 recipes from each cuisine\n",
    "bamboo_test[\"cuisine\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the training set by removing the test set from the **bamboo** data set, and name the training set **bamboo_train**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bamboo_test_index = bamboo.index.isin(bamboo_test.index)\n",
    "bamboo_train = bamboo[~bamboo_test_index]\n",
    "\n",
    "bamboo_train_ingredients = bamboo_train.iloc[:,1:] # ingredients\n",
    "bamboo_train_cuisines = bamboo_train[\"cuisine\"] # corresponding cuisines or labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that there are 30 _fewer_ recipes now for each cuisine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bamboo_train[\"cuisine\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the decision tree using the training set, **bamboo_train**, and name the generated tree **bamboo_train_tree** for prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bamboo_train_tree = tree.DecisionTreeClassifier(max_depth=15)\n",
    "bamboo_train_tree.fit(bamboo_train_ingredients, bamboo_train_cuisines)\n",
    "\n",
    "print(\"Decision tree model saved to bamboo_train_tree!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the decision tree and explore it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(bamboo_train_tree,\n",
    "                feature_names=list(bamboo_train_ingredients.columns.values),\n",
    "                out_file=\"bamboo_train_tree.dot\",\n",
    "                class_names=np.unique(bamboo_train_cuisines),\n",
    "                filled=True,\n",
    "                node_ids=True,\n",
    "                special_characters=True,\n",
    "                impurity=False,\n",
    "                label=\"all\",\n",
    "                leaves_parallel=False)\n",
    "\n",
    "with open(\"bamboo_train_tree.dot\") as bamboo_train_tree_image:\n",
    "    bamboo_train_tree_graph = bamboo_train_tree_image.read()\n",
    "graphviz.Source(bamboo_train_tree_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you defined our tree to be deeper, more decision nodes are generated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's test our model on the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bamboo_pred_cuisines = bamboo_train_tree.predict(bamboo_test_ingredients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quantify how well the decision tree is able to determine the cuisine of each recipe correctly, we will create a confusion matrix which presents a nice summary on how many recipes from each cuisine are correctly classified. It also sheds some light on what cuisines are being confused with what other cuisines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's go ahead and create the confusion matrix for how well the decision tree is able to correctly classify the recipes in **bamboo_test**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cuisines = np.unique(bamboo_test_cuisines)\n",
    "bamboo_confusion_matrix = confusion_matrix(bamboo_test_cuisines, bamboo_pred_cuisines, labels = test_cuisines)\n",
    "title = 'Bamboo Confusion Matrix'\n",
    "cmap = plt.cm.Blues\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "bamboo_confusion_matrix = (\n",
    "    bamboo_confusion_matrix.astype('float') / bamboo_confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    ) * 100\n",
    "\n",
    "plt.imshow(bamboo_confusion_matrix, interpolation='nearest', cmap=cmap)\n",
    "plt.title(title)\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(test_cuisines))\n",
    "plt.xticks(tick_marks, test_cuisines)\n",
    "plt.yticks(tick_marks, test_cuisines)\n",
    "\n",
    "fmt = '.2f'\n",
    "thresh = bamboo_confusion_matrix.max() / 2.\n",
    "for i, j in itertools.product(range(bamboo_confusion_matrix.shape[0]), range(bamboo_confusion_matrix.shape[1])):\n",
    "    plt.text(j, i, format(bamboo_confusion_matrix[i, j], fmt),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if bamboo_confusion_matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above code, you should get a confusion matrix similar to the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0103EN-SkillsNetwork/labs/Module%204/images/lab4_fig6_confusion_matrix.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows represent the actual cuisines from the dataset and the columns represent the predicted ones. Each row should sum to 100%. According to this confusion matrix, we make the following observations:\n",
    "\n",
    "* Using the first row in the confusion matrix, 60% of the **Chinese** recipes in **bamboo_test** were correctly classified by our decision tree whereas 37% of the **Chinese** recipes were misclassified as **Korean** and 3% were misclassified as **Indian**.\n",
    "\n",
    "* Using the Indian row, 77% of the **Indian** recipes in **bamboo_test** were correctly classified by our decision tree and 3% of the **Indian** recipes were misclassified as **Chinese** and 13% were misclassified as **Korean** and 7% were misclassified as **Thai**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note** that because decision trees are created using random sampling of the datapoints in the training set, then you may not get the same results every time you create the decision tree even using the same training set. The performance should still be comparable though! So don't worry if you get slightly different numbers in your confusion matrix than the ones shown above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the reference confusion matrix, how many **Japanese** recipes were correctly classified by our decision tree?\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Your Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here for a solution</summary>\n",
    "\n",
    "```python\n",
    "    #The correct answer is:\n",
    "    36.67%.\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also using the reference confusion matrix, how many **Korean** recipes were misclassified as **Japanese**?\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Your Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here for a solution</summary>\n",
    "\n",
    "```python\n",
    "    #The correct answer is:\n",
    "     3.33%.\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What cuisine has the least number of recipes correctly classified by the decision tree using the reference confusion matrix?\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Your Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here for a solution</summary>\n",
    "\n",
    "```python\n",
    "    #The correct answer is:\n",
    "     Japanese cuisine, with 36.67% only.\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "## Author\n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/aklson/\" target=\"_blank\">Alex Aklson</a>\n",
    "\n",
    "\n",
    "\n",
    "## Change Log\n",
    "\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2023-08-16 | 2.3| Patsy | Updated minor grammar items |\n",
    "| 2021-04-06 | 2.2 | Malika | Updated the data set link |\n",
    "| 2020-09-25  | 2.1  | Lakshmi  | Fixed typo errors |\n",
    "| 2020-08-27  | 2.0  | Lavanya  |  Moved lab to course repo in GitLab |\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "## <h3 align=\"center\"> Â© IBM Corporation 2020. All rights reserved. <h3/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
